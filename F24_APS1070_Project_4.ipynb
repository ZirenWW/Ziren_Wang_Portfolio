{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project 4, APS1070 Fall 2024\n",
        "#### **Linear Regression -  14 points**\n",
        "**Deadline: Nov 29th, 23:00**\n",
        "\n",
        "**Academic Integrity**\n",
        "\n",
        "This project is individual - it is to be completed on your own. If you have questions, please post your query in the APS1070 Piazza Q&A forums (the answer might be useful to others!).\n",
        "\n",
        "Do not share your code with others, or post your work online. Do not submit code that you have not written yourself without proper acknowledgment of the source, including generated code (please refer to the course syllabus). Students suspected of plagiarism on a project, midterm or exam will be referred to the department for formal discipline for breaches of the Student Code of Conduct."
      ],
      "metadata": {
        "id": "z5NG_Pj43DrX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please fill out the following:\n",
        "\n",
        "\n",
        "*   Your **name**:\n",
        "*   Your **student number**:"
      ],
      "metadata": {
        "id": "XWCGJ2uF3LJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to submit **(HTML + IPYNB)**\n",
        "\n",
        "1. Download your notebook: `File -> Download .ipynb`\n",
        "\n",
        "2. Click on the Files icon on the far left menu of Colab\n",
        "\n",
        "3. Select & upload your `.ipynb` file you just downloaded, and then obtain its path (right click) (you might need to hit the Refresh button before your file shows up)\n",
        "\n",
        "\n",
        "4. execute the following in a Colab cell:\n",
        "```\n",
        "%%shell\n",
        "jupyter nbconvert --to html /PATH/TO/YOUR/NOTEBOOKFILE.ipynb\n",
        "```\n",
        "\n",
        "5. An HTML version of your notebook will appear in the files, so you can download it.\n",
        "\n",
        "6. Submit **both** <font color='red'>`HTML` and `IPYNB`</font>  files for this notebook on Quercus for grading.\n",
        "\n",
        "\n",
        "Ref: https://stackoverflow.com/a/64487858"
      ],
      "metadata": {
        "id": "9sqDwcMs3PRQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4UwjkNZ3NGk"
      },
      "source": [
        "## Part 1 - Getting Started [2 marks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y62qieYVbct2"
      },
      "source": [
        "In this project, we are going to design a mathematical model that predicts electrical grid stability, as described in the paper below:\n",
        "\n",
        "*Sch√§fer, Benjamin, et al. 'Taming instabilities in power grid networks by decentralized control.' The European Physical Journal Special Topics 225.3 (2016): 569-582.*\n",
        "\n",
        "\n",
        "For this purpose, we are using the [Electrical Grid Stability](https://archive.ics.uci.edu/dataset/471/electrical+grid+stability+simulated+data) dataset which includes 10000 instances with 12 features describing the power grid's status. The goal is to unravel patterns and predict the `stab` column, which is a characteristic of the grid that represents its stability. Note that you can ignore the last column named `stabf` since this is a binary indicator showing whether the stability is above a certain threshold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVI7N2nWbct3"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\n",
        "    filepath_or_buffer=\"https://raw.githubusercontent.com/Sabaae/Dataset/main/electrical_grid_stability_simulated_data.csv\",\n",
        "    skipinitialspace=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKRMb9Ju3NGl"
      },
      "source": [
        "Here are the steps to complete this portion:\n",
        "\n",
        "1. Print the dataframe.\n",
        "2. Prepare your dataset as follows: **[1.5]**\n",
        "*   Using `train_test_split` from Sklearn, split the dataset into training and validation sets ($80\\%$ training, $20\\%$ validation). When splitting, set `random_state=111`. **[0.5]**\n",
        "*   Standardize the data **manually** according to the same method as `StandardScaler` from sklearn, but you may not directly use `StandardScaler'. **[0.5]**\n",
        "*   Insert a first column of all $1$s in both the training and validation data sets.**[0.5]**\n",
        "3. Explain why we should not insert the column of $1$s prior to standardization. **[0.5]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "barDBI-f5Q9s"
      },
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ6KBnbpbcuE"
      },
      "source": [
        "## Part 2 - Linear Regression Using Direct Solution [1 marks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qh9g1ead3NG-"
      },
      "source": [
        "Implement the direct solution of the linear regression problem on the training set. **[0.5]**\n",
        "\n",
        "**Note: You should use scipy.linalg.inv to perform the inversion, as numpy.linalg.inv may cause an error.**\n",
        "\n",
        "Report the root-mean-square error (RMSE) for both the training and validation sets. **[0.5]**\n",
        "\n",
        "You may use `root_mean_squared_error` from Sklearn for computing the RMSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzLJgLvy5Re4"
      },
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tceGCk1V3NG_"
      },
      "source": [
        "## Part 3 - Full Batch Gradient Descent [4 marks]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now implement a \"full batch\" gradient descent algorithm and record the training time for our model. Recall that the full batch gradient descent is,\n",
        " $$w_t = w_{t-1} - \\alpha~g_t$$ where $\\alpha$ is the learning rate and $g_t$ is your gradient, computed on the entire data.\n",
        "\n",
        "Here are the steps for this part:\n",
        "\n",
        "* Implement gradient descent for linear regression using a fixed learning rate of $\\alpha= 0.01$, and iterate until your model's **validation** RMSE converges.\n",
        "\n",
        "  We consider the gradient descent as having converged when RMSE on the validation set using gradient descent satisfies:\n",
        "\n",
        "  $$ RMSE_\\text{GD} \\leq 1.0005 \\times RMSE_\\text{Direct Solution}$$\n",
        "\n",
        "  where $RMSE_\\text{Direct Solution}$ is the RMSE on the validation set using the direct solution that you calculated in the previous part.\n",
        "\n",
        "  We refer to the quantity $RMSE_\\text{Direct Solution}\\times 1.0005$ as the convergence threshold (CT).\n",
        "  Be sure to compute the gradients yourself! Take a look at the code provided in the tutorial. **[1]**\n",
        "\n",
        "*  Record the training time (from the first iteration until convergence) using the `time.time()` function. **[0.5]**\n",
        "\n",
        "* Plot the training RMSE and the validation RMSE vs. epoch on the same figure.  **[1]**\n",
        "\n",
        "* Comment on overfitting/underfitting by observing the training and validation RMSE **[1]**\n",
        "\n",
        "    **Hint**: Initialize your weights with small random numbers (<$0.00001$) & please set **np.random.seed(1001)**\n",
        "\n",
        "* Explain the difference between epoch and iteration in the Gradient descent algorithm (SGD/mini-batch)? **[0.5]**"
      ],
      "metadata": {
        "id": "iQdtaXD6pVf9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3KOOJ8m5SAs"
      },
      "source": [
        "import time\n",
        "start_time = time.time() ## Records current time\n",
        "np.random.seed(1001)\n",
        "\n",
        "## GD Script -- Sample code in tutorial! ##\n",
        "\n",
        "print(\"--- Total Training Time: %s (s) ---\" % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3xwmhsr3NHS"
      },
      "source": [
        "## Part 4 - Mini-batch and Stochastic Gradient Descent [4 marks]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a function that performs mini-batch gradient descent until the convergence threshold (CT) is reached. **[1]**\n",
        "\n",
        "The inputs of that function are:\n",
        "  * Input data (training/validation), Batch size, Learning Rate, Convergence Threshold (CT)\n",
        "\n",
        "Your function will return the following arrays:\n",
        " * The final weights after training.\n",
        " * The training RMSE at each epoch.\n",
        " * The validation RMSE at each epoch.\n",
        " * An array that has the elapsed time from the start of the training process to the end of each epoch (e.g., if each epoch takes exactly 2 seconds, the array would look like: [2 4 6 8 ...]).\n",
        "\n",
        "Notes:\n",
        "* use **np.random.seed(1001)** before setting the initial (random) weights\n",
        "\n",
        "  ```\n",
        "  def mini_batch_gd(YOUR_INPUTS):\n",
        "    np.random.seed(1001)\n",
        "\n",
        "    ### YOUR CODE HERE ###\n",
        "    \n",
        "    return YOUR_OUTPUTS\n",
        "  ```\n",
        "\n",
        "* For certain batch sizes, GD might not converge to a solution. For that reason, you need to check the RMSE of the validation/training set at each epoch, and if it's getting larger and larger, you should stop the training for that case (the design is up to you!).\n",
        "* CT will help you to know when your model is converged.\n",
        "* **Important: after each epoch, you need to shuffle the entire training set.** This ensures that new mini-batches are selected for every epoch. Hint: use `np.random.permutation`.\n",
        "\n",
        "Let's now use the function to investigate the effect of batch size on convergence. When the batch size is 1, we call that stochastic gradient descent. When the batch size equals the # of training data, it is full-batch (i.e., all data points are used at every iteration). Anywhere in between is mini-batch (we use some of the data).\n",
        "  * Sweep different values for the mini-batch size (at least 5 values that **converge**), each time using a learning rate of $\\alpha= 0.01$. **Hint: Try batch sizes that are powers of two (e.g., 2,4,8,16,...).** These batch sizes fit better on the hardware and may achieve higher performance.  **[0.5]**\n",
        "\n",
        "Provide the following $3$ plots:\n",
        "  1.  Plot training and validation RMSE vs. **epoch** for all the **converging** batch sizes (some batch sizes might not converge) in a figure. The X-axis is Epoch # and the Y-axis is RMSE. **[0.5]**\n",
        "  2. Plot training and validation RMSE vs. **time** for all the **converging** batch sizes in a figure. The X-axis is Time, and the Y-axis is RMSE. **[0.5]**\n",
        "  3. Plot Total training time (y-axis) vs. Batch size (x-axis). **[0.5]**\n",
        "\n",
        "Describe your findings, including the main takeaways from each of your plots. **[1]**"
      ],
      "metadata": {
        "id": "Mxq64TAhpaZ9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGQ4_OPkleDq"
      },
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5 - Gradient Descent Learning Rate [3 marks]\n",
        "Use the function that you designed in Part 4 to investigate the effect of learning rate on convergence.\n",
        "\n",
        "1. Pick 3 batch sizes that do not converge with a learning rate of $\\alpha= 0.01$ (last part) and try to find a learning rate that results in convergence for each batch size. Report your findings in a table. Mention batch size, learning rate, and training and validation RMSE. **[1]**\n",
        "\n",
        "2. Select the best batch size based on Part 4's fastest convergence time and sweep the learning rate (at least 10 values, all should converge) while applying Mini-batch GD.\n",
        "  \n",
        "  * Plot the training and validation RMSE vs. **epoch** (x-axis) for all the learning rates that you tried in a figure. **[0.5]**\n",
        "  * Plot the training and validation RMSE vs. **time** (x-axis) for all the learning rates that you tried in a figure. **[0.5]**\n",
        "  * Describe your findings, including the main takeaways from each of your plots. **[1]**"
      ],
      "metadata": {
        "id": "oITEHnmO3nbD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f-s_HRffwlR"
      },
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}